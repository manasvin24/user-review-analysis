{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs dependencies and imports required libraries.\n",
    "%pip install -U transformers\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"deepset/tinyroberta-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc # Import garbage collector module\n",
    "\n",
    "# 1. Import dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/BDFoodSent-334k.csv', encoding='utf-8')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please check file name...\")\n",
    "    exit()\n",
    "\n",
    "REVIEW_COLUMN = 'text' # Ensure column name is correct\n",
    "if REVIEW_COLUMN not in df.columns:\n",
    "    print(f\"Error: There is no '{REVIEW_COLUMN}' column in your data set...\")\n",
    "    exit()\n",
    "\n",
    "# 2. Define aspects and questions for extraction\n",
    "aspect_questions = {\n",
    "    'Taste_Aspect': 'What is the opinion about the taste, quantity or food quality?',\n",
    "    'Service_Aspect': 'What is the feedback on the service or delivery?',\n",
    "    'Ambiance_Aspect': 'What is the description of the restaurant\\'s atmosphere or environment?'\n",
    "}\n",
    "\n",
    "# 3. Set Batch Size\n",
    "# This is crucial. The number of reviews processed in each batch.\n",
    "# Start with a small number, e.g., 50 or 100. Decrease if memory crashes continue.\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# 4. Initialize an empty list to store results from all batches\n",
    "results_list = []\n",
    "\n",
    "# 5. Loop and process data in batches\n",
    "print(f\"Starting batch processing with batch size: {BATCH_SIZE}\")\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "    # Determine the start and end index for the current batch\n",
    "    batch_start = i\n",
    "    batch_end = i + BATCH_SIZE\n",
    "\n",
    "    # Extract the reviews for the current batch\n",
    "    # Use loc for reliable slicing\n",
    "    batch_reviews = df.loc[batch_start:batch_end-1, REVIEW_COLUMN].tolist()\n",
    "\n",
    "    # Initialize a list of dictionaries to store results for the current batch\n",
    "    batch_results = []\n",
    "\n",
    "    # Process each review within the batch\n",
    "    for review in batch_reviews:\n",
    "        row_result = {}\n",
    "        for aspect, question in aspect_questions.items():\n",
    "            try:\n",
    "                # Use the previously loaded 'pipe' model\n",
    "                res = pipe({\n",
    "                    'question': question,\n",
    "                    'context': review\n",
    "                })\n",
    "                row_result[aspect + '_Answer'] = res['answer']\n",
    "                row_result[aspect + '_Score'] = res['score']\n",
    "            except Exception as e:\n",
    "                # Handle any errors during model inference\n",
    "                row_result[aspect + '_Answer'] = f\"Error: {e}\"\n",
    "                row_result[aspect + '_Score'] = 0.0\n",
    "\n",
    "        batch_results.append(row_result)\n",
    "\n",
    "    # Append current batch results to the master results list\n",
    "    results_list.extend(batch_results)\n",
    "\n",
    "    # Force garbage collection to free up memory\n",
    "    gc.collect()\n",
    "\n",
    "# 6. Convert the results list to a DataFrame\n",
    "extracted_df = pd.DataFrame(results_list)\n",
    "\n",
    "# 7. Merge the extracted results with the original data (ensure equal length)\n",
    "# Use df.reset_index(drop=True) to ensure a clean merge on index\n",
    "final_df = pd.concat([df.reset_index(drop=True), extracted_df], axis=1)\n",
    "\n",
    "# 8. Export results to a new CSV file (same as before)\n",
    "OUTPUT_FILENAME = 'analyzed_reviews_batch_output.csv'\n",
    "final_df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")\n",
    "print(f\"Results successfully saved to file: {OUTPUT_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde71da",
   "metadata": {},
   "source": [
    "Note: After the QA model is applied, the resulting new dataset consists of six additional columns representing [aspect + '_Answer'] and [aspect + '_Score']. Here, the aspect includes Taste, Service, and Ambiance, and the score represents the model's confidence score for the answer.\n",
    "\n",
    "*** However, since the confidence score cannot objectively reflect the relevance between the answer and the question, a semantic relevance model is introduced below for further analysis. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Load Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive load done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Setup Paths (Assumed to be defined/loaded) ---\n",
    "DRIVE_PATH = '/content/drive/MyDrive/'\n",
    "PROJECT_FOLDER = 'Colab Notebooks/'\n",
    "\n",
    "# 1. Define possible input file paths (Primary and Fallback)\n",
    "PRIMARY_INPUT_FILE = os.path.join(DRIVE_PATH, PROJECT_FOLDER, 'analyzed_reviews_batch_output.csv')\n",
    "\n",
    "# Output file for the Similarity Scores\n",
    "OUTPUT_FILENAME_FINAL_MAX_DRIVE = os.path.join(DRIVE_PATH, PROJECT_FOLDER, 'final_analysis_with_similarity_wide.csv')\n",
    "\n",
    "# 2. Define sub-topic questions and their main aspects\n",
    "# NOTE: Reverting to the original single, composite question per main aspect for similarity calculation.\n",
    "ALL_ASPECTS = {\n",
    "    'Food_Aspect': [\n",
    "        ('Composite', 'What is the opinion about the taste, quantity or food quality?')\n",
    "    ],\n",
    "    'Service_Aspect': [\n",
    "        ('Composite', 'What is the feedback on the service or delivery?')\n",
    "    ],\n",
    "    'Ambiance_Aspect': [\n",
    "        ('Composite', 'What is the description of the restaurant\\'s atmosphere or environment?')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Define original answer columns corresponding to each main aspect (Keep unchanged)\n",
    "ANSWER_COL_MAP = {\n",
    "    'Food_Aspect': 'Taste_Aspect_Answer',\n",
    "    'Service_Aspect': 'Service_Aspect_Answer',\n",
    "    'Ambiance_Aspect': 'Ambiance_Aspect_Answer'\n",
    "}\n",
    "\n",
    "# 4. Robust Data Loading Block\n",
    "try:\n",
    "    # Attempt to load Primary file, include low_memory=False to solve parsing errors\n",
    "    df_results = pd.read_csv(PRIMARY_INPUT_FILE, encoding='utf-8', low_memory=False)\n",
    "    print(f\"Data successfully loaded from: {PRIMARY_INPUT_FILE}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Input file not found at {PRIMARY_INPUT_FILE}. Please check your Drive path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Failed to parse CSV file due to an error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 5. Initialize new score storage\n",
    "new_max_scores = {}\n",
    "for aspect in ALL_ASPECTS.keys():\n",
    "    new_max_scores[f'{aspect}_Similarity_Score'] = []\n",
    "\n",
    "# 6. Batch Similarity Calculation (Revised: Prevents scores from being incorrectly set to 0 upon failure)\n",
    "BATCH_SIZE = 512\n",
    "print(\"Starting Final Similarity Calculation (3 Main Aspects)...\")\n",
    "\n",
    "# Track the count of rows where calculation failed (and score was set to 0)\n",
    "failed_calculation_count = 0\n",
    "\n",
    "for index, row in tqdm(df_results.iterrows(), total=len(df_results), desc=\"Calculating Max Similarity\"):\n",
    "\n",
    "    for main_aspect, sub_topics in ALL_ASPECTS.items():\n",
    "\n",
    "        answer_col_name = ANSWER_COL_MAP[main_aspect]\n",
    "        answer = row.get(answer_col_name)\n",
    "\n",
    "        questions = [q for _, q in sub_topics]\n",
    "        current_max_sim = 0.0 # Default score is 0.0\n",
    "\n",
    "        # --- A. Check answer validity ---\n",
    "        # Only enter calculation block if the answer is valid\n",
    "        if not pd.isna(answer) and \"Error:\" not in str(answer) and answer != \"\":\n",
    "            try:\n",
    "                # Encode all sub-questions and the single answer in a batch\n",
    "                sentences = questions + [answer]\n",
    "\n",
    "                # ****** CRITICAL STEP: Model Calculation ******\n",
    "                embeddings = model.encode(sentences, batch_size=BATCH_SIZE, convert_to_tensor=True).cpu()\n",
    "\n",
    "                # Calculate similarity and find the maximum score\n",
    "                q_embeddings = embeddings[:-1]\n",
    "                a_embedding = embeddings[-1]\n",
    "                sims = util.cos_sim(q_embeddings, a_embedding).flatten().tolist()\n",
    "\n",
    "                current_max_sim = max(sims)\n",
    "                # ****** If calculation succeeds, current_max_sim > 0 ******\n",
    "\n",
    "            except Exception as e:\n",
    "                # B. If calculation fails, record and keep 0.0\n",
    "                current_max_sim = 0.0\n",
    "                # Directly modify global counter (fix NameError)\n",
    "                # Assuming this variable is at the top level of the script, we can access it directly\n",
    "                global failed_calculation_count\n",
    "                failed_calculation_count += 1\n",
    "                # Only print failed row for diagnosing memory/data issues\n",
    "                print(f\"\\nCalculation failed at index {index} for {main_aspect}: {e}\")\n",
    "\n",
    "        # Store the final MAX score in the dictionary\n",
    "        new_max_scores[f'{main_aspect}_Similarity_Score'].append(current_max_sim)\n",
    "\n",
    "    if index % 1000 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "# Additional diagnostic information\n",
    "total_attempts = len(df_results) * 3\n",
    "print(f\"\\nTotal rows where calculation failed (and score was set to 0.0): {failed_calculation_count}\")\n",
    "print(f\"Percentage of failed calculations: {failed_calculation_count / total_attempts * 100:.4f}%\")\n",
    "# 7. Merge the new MAX similarity score columns\n",
    "df_new_scores = pd.DataFrame(new_max_scores)\n",
    "\n",
    "# To prevent index misalignment during merge, we use reset_index\n",
    "df_final = pd.concat([df_results.reset_index(drop=True), df_new_scores.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 8. Export final results\n",
    "df_final.to_csv(OUTPUT_FILENAME_FINAL_MAX_DRIVE, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")\n",
    "print(f\"Final data (with 3 Similarity Scores) saved to: {OUTPUT_FILENAME_FINAL_MAX_DRIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8c899",
   "metadata": {},
   "source": [
    "Note: At this point, the analysis of the QA model and similarity model has been completed. To facilitate subsequent data analysis, the dataset will be simplified, retaining only the necessary and useful columns and discarding unnecessary data to reduce the dataset file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41251eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os # Used for file operations\n",
    "\n",
    "# --- Assume Drive_PATH and PROJECT_FOLDER are defined in previous code ---\n",
    "DRIVE_PATH = '/content/drive/MyDrive/'\n",
    "PROJECT_FOLDER = 'Colab Notebooks/'\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# 1. Define input and output file paths\n",
    "INPUT_FILENAME_DRIVE = DRIVE_PATH + PROJECT_FOLDER + 'final_analysis_with_similarity_wide.csv'\n",
    "OUTPUT_FILENAME_SIMPLIFIED_DRIVE = DRIVE_PATH + PROJECT_FOLDER + 'Most_Simplified_analysis_results.csv'\n",
    "\n",
    "print(f\"Input file path set to: {INPUT_FILENAME_DRIVE}\")\n",
    "print(f\"Simplified output file path set to: {OUTPUT_FILENAME_SIMPLIFIED_DRIVE}\")\n",
    "\n",
    "# 2. Define target columns\n",
    "FINAL_COLUMNS = [\n",
    "    'text', 'name', 'city',\n",
    "\n",
    "    # Taste Aspect\n",
    "    'Taste_Aspect_Answer',\n",
    "    'Taste_Aspect_Similarity_Score',\n",
    "\n",
    "    # Service Aspect\n",
    "    'Service_Aspect_Answer',\n",
    "    'Service_Aspect_Similarity_Score',\n",
    "\n",
    "    # Ambiance Aspect\n",
    "    'Ambiance_Aspect_Answer',\n",
    "    'Ambiance_Aspect_Similarity_Score'\n",
    "]\n",
    "\n",
    "# 3. Read input file\n",
    "try:\n",
    "    # Only read the columns we need to save memory\n",
    "    df_results = pd.read_csv(INPUT_FILENAME_DRIVE, encoding='utf-8')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found at {INPUT_FILENAME_DRIVE}. Please check the path.\")\n",
    "    # Check if Drive is mounted or the path is correct\n",
    "    if not os.path.exists(DRIVE_PATH):\n",
    "        print(\"Hint: Google Drive does not seem to be mounted successfully. Please run drive.mount('/content/drive').\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 4. Filter DataFrame size, only keeping required columns\n",
    "# Check which target columns actually exist in the loaded DataFrame\n",
    "columns_to_keep = [col for col in FINAL_COLUMNS if col in df_results.columns]\n",
    "\n",
    "if len(columns_to_keep) < 9:\n",
    "    print(f\"\\nWarning: Could not find all 9 target columns. Only {len(columns_to_keep)} columns were found.\")\n",
    "\n",
    "# Filter and order by the sequence defined in FINAL_COLUMNS\n",
    "df_simplified = df_results[columns_to_keep]\n",
    "\n",
    "# 5. Rename columns to reflect the 'Food' aspect meaning\n",
    "df_simplified = df_simplified.rename(columns={\n",
    "    'Taste_Aspect_Answer': 'Food_Aspect_Answer',\n",
    "    'Taste_Aspect_Similarity_Score': 'Food_Aspect_Similarity_Score'\n",
    "})\n",
    "\n",
    "# 6. Export simplified results\n",
    "df_simplified.to_csv(OUTPUT_FILENAME_SIMPLIFIED_DRIVE, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n--- Processing Complete ---\")\n",
    "print(f\"File read and filtered successfully.\")\n",
    "print(f\"Simplified results (containing only {len(columns_to_keep)} columns) saved to: {OUTPUT_FILENAME_SIMPLIFIED_DRIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809aa2c4",
   "metadata": {},
   "source": [
    "Note: At this point, we have a relatively concise instruction set. However, the original dataset is still enormous. To further refine the data and make subsequent analysis more effective, we decided to include only the top 10% of answers with the highest question-answer similarity scores in the analysis.\n",
    "\n",
    "Before this, since our analysis focuses on English text, we removed non-English text and stipulated that valid text must contain at least 3 English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ad093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langdetect library\n",
    "%pip install langdetect\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "tqdm.pandas(desc=\"Language Detection Progress\")\n",
    "\n",
    "INPUT_FILENAME_DRIVE = DRIVE_PATH + PROJECT_FOLDER + 'Most_Simplified_analysis_results.csv'\n",
    "OUTPUT_FILENAME_DRIVE = DRIVE_PATH + PROJECT_FOLDER + 'Filtered_English_Texts.csv'\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 1: Load dataset\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    # Name the loaded data df_to_analyze to match your filtering code logic\n",
    "    df_to_analyze = pd.read_csv(INPUT_FILENAME_DRIVE)\n",
    "    print(f\"\\nData loaded successfully! Total rows: {len(df_to_analyze)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found. Please check the path: {INPUT_FILENAME_DRIVE}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nError occurred while loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 2: Define language detection function\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def safe_detect_language(text):\n",
    "    \"\"\"Attempt to detect text language, return 'unknown' on failure\"\"\"\n",
    "    # Ensure text is a non-empty string\n",
    "    if pd.isna(text) or not isinstance(text, str) or not text.strip():\n",
    "         return 'na'\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        # Catch text whose language cannot be detected (usually too short or contains non-Latin characters)\n",
    "        return 'unknown'\n",
    "\n",
    "print(\"\\n--- Starting data cleaning and filtering ---\")\n",
    "print(f\"Data volume before cleaning: {len(df_to_analyze)}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 3: Language filtering (keep English 'en')\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# 1. Create new column and detect language\n",
    "# Use .progress_apply to enable progress bar\n",
    "df_to_analyze['language'] = df_to_analyze['text'].progress_apply(safe_detect_language)\n",
    "\n",
    "# 2. Filter to keep only English text\n",
    "df_to_analyze_english = df_to_analyze[df_to_analyze['language'] == 'en'].copy()\n",
    "\n",
    "print(f\"Data volume after filtering non-English: {len(df_to_analyze_english)}\")\n",
    "\n",
    "# Check if any data remains\n",
    "if len(df_to_analyze_english) == 0:\n",
    "    print(\"Warning: No English text remained after filtering, script stopped.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 4: Length filtering (word count > 3)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Filtering logic: Use spaces to split text, calculate word count, and require word count > 3\n",
    "# This step does not require a progress bar as it is a simple string operation\n",
    "df_filtered = df_to_analyze_english[\n",
    "    df_to_analyze_english['text'].apply(lambda x: len(str(x).split()) > 3)\n",
    "].copy()\n",
    "\n",
    "print(f\"Final data volume after filtering length â‰¤ 3 words: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 5: Save final results\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n--- Step 5: Save final filtered results to Drive ---\")\n",
    "\n",
    "# Save the final filtered dataset\n",
    "df_filtered.to_csv(OUTPUT_FILENAME_DRIVE, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Filtered data successfully saved to Google Drive:\")\n",
    "print(f\"Output file path is: {OUTPUT_FILENAME_DRIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d19f8",
   "metadata": {},
   "source": [
    "*** The following will extract the first 10% of the data. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99683632",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILENAME_DRIVE = DRIVE_PATH + PROJECT_FOLDER + 'Filtered_English_Texts.csv'\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 2: Load dataset\n",
    "# -------------------------------------------------------------------\n",
    "# Assuming the dataset uses comma delimiters\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILENAME_DRIVE)\n",
    "    print(f\"\\nData loaded successfully! Total rows: {len(df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found. Please check the path: {INPUT_FILENAME_DRIVE}\")\n",
    "    # Stop if file is not found\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nError occurred while loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 3: Extract the top 10% of data for the three aspects\n",
    "# -------------------------------------------------------------------\n",
    "# Define column names for the three aspect similarity scores\n",
    "score_columns = [\n",
    "    'Food_Aspect_Similarity_Score',\n",
    "    'Service_Aspect_Similarity_Score',\n",
    "    'Ambiance_Aspect_Similarity_Score'\n",
    "]\n",
    "\n",
    "# Dictionary to store the three resulting DataFrames\n",
    "top_10_data = {}\n",
    "\n",
    "print(\"\\n--- Starting calculation and filtering for Top 10% data ---\")\n",
    "\n",
    "for col in score_columns:\n",
    "    # 1. Calculate the Top 10% threshold (90th percentile)\n",
    "    if col not in df.columns:\n",
    "        print(f\"Error: Column name '{col}' does not exist in the dataset, skipping this aspect.\")\n",
    "        continue\n",
    "\n",
    "    threshold = df[col].quantile(0.9)\n",
    "    print(f\"Aspect '{col}' Top 10% threshold is: {threshold:.4f}\")\n",
    "\n",
    "    # 2. Filter: Select all rows where the score is greater than or equal to the threshold\n",
    "    filtered_df = df[df[col] >= threshold].copy()\n",
    "\n",
    "    # 3. Store the result, and rename columns for distinction when saving\n",
    "    # Note: The .rename() here is commented out and will keep the original column names\n",
    "    result_df = filtered_df#.rename(columns={col: 'Final_Similarity_Score'})\n",
    "\n",
    "    # 4. Add a column to identify the aspect\n",
    "    aspect_name = col.split('_')[0] # Extract 'Food', 'Service', or 'Ambiance'\n",
    "    result_df['Aspect'] = aspect_name\n",
    "\n",
    "    top_10_data[aspect_name] = result_df\n",
    "\n",
    "    print(f\"Aspect '{aspect_name}' extracted {len(filtered_df)} rows of data.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 4: Save results separately to the specified output path (Modified)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Step 4: Save the Top 10% results for each aspect separately ---\")\n",
    "\n",
    "saved_files = []\n",
    "for aspect, df_aspect in top_10_data.items():\n",
    "    # Construct a unique output filename: e.g., Top_10%_Food_results.csv\n",
    "    output_filename = f'English_Top_10%_{aspect}_results.csv'\n",
    "    output_filepath = DRIVE_PATH + PROJECT_FOLDER + output_filename\n",
    "\n",
    "    # Save DataFrame\n",
    "    df_aspect.to_csv(output_filepath, index=False, encoding='utf-8')\n",
    "    saved_files.append(output_filepath)\n",
    "    print(f\"Aspect '{aspect}' with {len(df_aspect)} rows of data saved to: {output_filepath}\")\n",
    "\n",
    "print(f\"\\nAll Top 10% data has been saved separately into {len(saved_files)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c54d7c",
   "metadata": {},
   "source": [
    "Note: At this point, all the effective data has been extracted. The final step is sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc43381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Define the three aspects to be processed\n",
    "ASPECTS = ['Food', 'Service', 'Ambiance']\n",
    "BASE_FILENAME = 'English_Top_10%_{}_results.csv'\n",
    "\n",
    "# --- Model Initialization ---\n",
    "MODEL_NAME = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "print(\"\\n--- Sentiment Analysis Model Initialization ---\")\n",
    "\n",
    "# Try to use GPU (device ID 0), otherwise use CPU (-1)\n",
    "device = 0 if torch.cuda.is_available() and torch.cuda.device_count() > 0 else -1\n",
    "print(f\"Device in use: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "try:\n",
    "    classifier = pipeline(\n",
    "        'sentiment-analysis',\n",
    "        model=MODEL_NAME,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed. Please check if transformers and torch libraries are installed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 2: Loop through and analyze each aspect file\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "for aspect in ASPECTS:\n",
    "    print(f\"\\n==================== Starting processing aspect: {aspect} ====================\")\n",
    "\n",
    "    # Construct input and output file paths\n",
    "    input_file = DRIVE_PATH + PROJECT_FOLDER + BASE_FILENAME.format(aspect)\n",
    "    # New output file to distinguish it from the original Top 10% file\n",
    "    output_file = DRIVE_PATH + PROJECT_FOLDER + f'English_Top_10%_{aspect}_sentiment.csv'\n",
    "\n",
    "    # A. Load Data\n",
    "    try:\n",
    "        df_aspect = pd.read_csv(input_file)\n",
    "        if 'text' not in df_aspect.columns:\n",
    "             print(f\"FATAL ERROR: Missing 'text' column in file '{input_file}'. Skipping.\")\n",
    "             continue\n",
    "        print(f\"File loaded successfully. Data size: {len(df_aspect)} records.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found: {input_file}. Skipping this aspect.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading file: {e}. Skipping this aspect.\")\n",
    "        continue\n",
    "\n",
    "    # B. Perform Batch Sentiment Analysis (with progress bar)\n",
    "    texts_to_analyze = df_aspect['text'].tolist()\n",
    "    sentiment_results = []\n",
    "    total_texts = len(texts_to_analyze)\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    print(f\"Performing batch sentiment analysis on {total_texts} texts...\")\n",
    "\n",
    "    # Use tqdm loop for batch inference\n",
    "    for i in tqdm(range(0, total_texts, BATCH_SIZE), desc=f\"{aspect} Sentiment Analysis Progress\"):\n",
    "        batch = texts_to_analyze[i:i + BATCH_SIZE]\n",
    "        results_batch = classifier(batch)\n",
    "        sentiment_results.extend(results_batch)\n",
    "\n",
    "    print(\"Sentiment analysis complete. Integrating results...\")\n",
    "\n",
    "    # C. Integrate Results\n",
    "    df_aspect['Sentiment_Label'] = [res['label'] for res in sentiment_results]\n",
    "    df_aspect['Sentiment_Confidence'] = [res['score'] for res in sentiment_results]\n",
    "\n",
    "    # Print result example\n",
    "    print(df_aspect[['text', 'Sentiment_Label', 'Sentiment_Confidence']].head())\n",
    "\n",
    "    # D. Save final results to Drive\n",
    "    df_aspect.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"\\nResults successfully saved to new file: {output_file}\")\n",
    "\n",
    "print(\"\\n\\n==================== Done. ====================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
